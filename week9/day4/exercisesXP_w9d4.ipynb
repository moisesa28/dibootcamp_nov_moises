{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Exercises XP\n",
        "\n"
      ],
      "metadata": {
        "id": "na5h6IR9ZQdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1: Calculating Required Sample Size**\n",
        "\n",
        "You are planning an A/B test to evaluate the impact of a new email subject line on the open rate. Based on past data, you expect a small effect size of 0.3 (an increase from 20% to 23% in the open rate). You aim for an 80% chance (power = 0.8) of detecting this effect if it exists, with a 5% significance level (α = 0.05).\n",
        "\n",
        "Calculate the required sample size per group using Python’s statsmodels library.\n",
        "What sample size is needed for each group to ensure your test is properly powered?"
      ],
      "metadata": {
        "id": "9CdAffTbZV3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVtUVeJ-ZFnk",
        "outputId": "43af1ae3-2d86-4bea-8af4-0e22ae7970fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 175.38\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Define the parameters\n",
        "effect_size = 0.3\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "\n",
        "# Calculate the sample size\n",
        "analysis = TTestIndPower()\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f'Required sample size per group: {sample_size:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2: Understanding the Relationship Between Effect Size and Sample Size**\n",
        "\n",
        "Using the same A/B test setup as in Exercise 1, you want to explore how changing the expected effect size impacts the required sample size.\n",
        "\n",
        "Calculate the required sample size for the following effect sizes: 0.2, 0.4, and 0.5, keeping the significance level and power the same.\n",
        "How does the sample size change as the effect size increases? Explain why this happens."
      ],
      "metadata": {
        "id": "-IeHkjRpZ3io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#effect 0.2\n",
        "effect_size = 0.2\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "\n",
        "# Calculate the sample size\n",
        "analysis = TTestIndPower()\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f'Required sample size per group: {sample_size:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0jBZHYuaCeE",
        "outputId": "ff4d9791-8475-4d96-c4dc-31ddb3af71a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 393.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#effect 0.4\n",
        "effect_size = 0.4\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "\n",
        "# Calculate the sample size\n",
        "analysis = TTestIndPower()\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f'Required sample size per group: {sample_size:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn37yqGDaO8p",
        "outputId": "acac9ee7-8352-44fc-9c6d-ccaf29f71fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 99.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#effect 0.5\n",
        "effect_size = 0.5\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "\n",
        "# Calculate the sample size\n",
        "analysis = TTestIndPower()\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f'Required sample size per group: {sample_size:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HLdn9RKaTXK",
        "outputId": "1163b103-a66d-4241-dd75-11803b390cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 63.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "As the desire effect grows, the need for the sample size decreases, which makes it easier to detect a difference between the groups"
      ],
      "metadata": {
        "id": "vKKNFiz6aawB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3: Exploring the Impact of Statistical Power**\n",
        "\n",
        "Imagine you are conducting an A/B test where you expect a small effect size of 0.2. You initially plan for a power of 0.8 but wonder how increasing or decreasing the desired power level impacts the required sample size.\n",
        "\n",
        "Calculate the required sample size for power levels of 0.7, 0.8, and 0.9, keeping the effect size at 0.2 and significance level at 0.05.\n",
        "Question: How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?"
      ],
      "metadata": {
        "id": "rdIES6WGa7z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#power level 0.7\n",
        "effect_size = 0.2\n",
        "alpha = 0.05\n",
        "power = 0.7\n",
        "\n",
        "# Calculate the sample size\n",
        "analysis = TTestIndPower()\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f'Required sample size per group: {sample_size:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa9BzggYbFVK",
        "outputId": "421930a1-a843-4c9c-a2aa-1276ca4b196e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 309.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#power level of 0.8\n",
        "effect_size = 0.2\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "\n",
        "# Calculate the sample size\n",
        "analysis = TTestIndPower()\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f'Required sample size per group: {sample_size:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9D8bvqfbNmi",
        "outputId": "a2e40333-b234-40df-9bd1-5fbdef2ccfbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 393.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#power level of 0.9\n",
        "effect_size = 0.2\n",
        "alpha = 0.05\n",
        "power = 0.9\n",
        "\n",
        "# Calculate the sample size\n",
        "analysis = TTestIndPower()\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f'Required sample size per group: {sample_size:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAKHEdAxbTgE",
        "outputId": "fd697838-f53d-44ab-e9f2-998c33887e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 526.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "As the power level increases, so does the sample size, which means higher power means you want to be more confident in detecting a true effect (if one exists). To achieve this higher confidence, you need more data, which translates to a larger sample size.\n",
        "\n",
        "It is important to understand it to:\n",
        "**Avoiding Underpowered Tests,**\n",
        "**Resource Efficiency**, and\n",
        "**Informed Decision-Making**,"
      ],
      "metadata": {
        "id": "j3MppO_Jbomx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4: Implementing Sequential Testing**\n",
        "\n",
        "You are running an A/B test on two versions of a product page to increase the purchase rate. You plan to monitor the results weekly and stop the test early if one version shows a significant improvement.\n",
        "\n",
        "Define your stopping criteria.\n",
        "Decide how you would implement sequential testing in this scenario.\n",
        "At the end of week three, Version B has a p-value of 0.02. What would you do next?\n"
      ],
      "metadata": {
        "id": "C2znpzWld5BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "Before starting the A/B test, you must clearly define when you are allowed to stop it, such as the minimum test duration, the minimum sample size, and how strong the improvement must be to matter. Because results are checked every week, you should use a sequential testing method (like alpha-spending or Bayesian testing) so that early checks don’t lead to false wins. At the end of week three, a p-value of 0.02 is only enough to stop the test if it meets the pre-defined sequential threshold; if it does, you can confidently launch Version B, but if not, you should continue the test until the planned end."
      ],
      "metadata": {
        "id": "Sv2IRjz3fNil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5: Applying Bayesian A/B Testing**\n",
        "\n",
        "You’re testing a new feature in your app, and you want to use a Bayesian approach. Initially, you believe the new feature has a 50% chance of improving user engagement. After collecting data, your analysis suggests a 65% probability that the new feature is better.\n",
        "\n",
        "Describe how you would set up your prior belief.\n",
        "After collecting data, how does the updated belief (posterior distribution) influence your decision?\n",
        "What would you do if the posterior probability was only 55%?"
      ],
      "metadata": {
        "id": "DA21gERwgR36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "In a Bayesian A/B test, I would start with a neutral prior that reflects a 50/50 belief that the new feature could be better or worse, meaning I don’t favor either version before seeing data. After collecting data, the updated belief (posterior) guides decisions: a 65% probability that the feature is better suggests it is promising but usually not strong enough to fully launch unless it meets a pre-defined confidence threshold. If the posterior probability were only 55%, I would treat the result as inconclusive and either collect more data or stop the test for futility, since the evidence for improvement is weak."
      ],
      "metadata": {
        "id": "L3GVmXuTgos8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6: Implementing Adaptive Experimentation**\n",
        "\n",
        "You’re running a test with three different website layouts to increase user engagement. Initially, each layout gets 33% of the traffic. After the first week, Layout C shows higher engagement.\n",
        "\n",
        "Explain how you would adjust the traffic allocation after the first week.\n",
        "Describe how you would continue to adapt the experiment in the following weeks.\n",
        "What challenges might you face with adaptive experimentation, and how would you address them?"
      ],
      "metadata": {
        "id": "KmoAYVHKg5YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "After the first week, I would move some traffic toward Layout C since it shows higher engagement, but I wouldn’t send all users there yet. I would keep a smaller but steady amount of traffic on Layouts A and B so I can continue learning and make sure the early result wasn’t just noise. Over the next weeks, I would keep updating the results and gradually send more traffic to the layout that continues to perform best, while setting clear rules for when to stop the test or pause a layout that performs poorly.\n",
        "\n",
        "Adaptive experiments can be harder to manage because early results can be misleading, traditional statistics don’t always apply, and traffic or user behavior can change over time. To handle this, I would move traffic gradually, keep minimum traffic levels for all layouts, use methods designed for adaptive testing, and set guardrails to protect user experience."
      ],
      "metadata": {
        "id": "uir_VAYShOPH"
      }
    }
  ]
}